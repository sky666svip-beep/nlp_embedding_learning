# 当前工作进展 (Progress)

## 2026-02-25

- [x] 初始化项目目录 `nlp_embedding_learning`
- [x] 创建 Manus 风格的规划文件 `task_plan.md`, `findings.md`, `progress.md`
- [x] 制定具体的技术设计方案（语言、框架、手搭架构细节）
- [x] 撰写交互式教学展示界面设计

## 2026-02-25 (当前进展)

- **数据集准备**：生成并确立 `lcqmc_mini.csv` 作为轻量级的开源训练评测数据。
- **环境搭建**：产出 `requirements.txt`。
- **当前进展**：已完全实现核心的基础模型结构与数据加载；已完成交互式的 Streamlit 教学页面；并在 `docs/` 下输出了增量开发文档。
- **阶段完成**：第一阶、第二阶、第三阶构建与第五阶段档产出已全部完成，可进行功能演示。

## 2026-02-25 (增量开发一)

- **目标**：为项目引入 GPU 加速 (MPS/CUDA) 支持，以提升模型的训练和推理速度。
- **当前进展**：已在 `train.py` 训练循环中注入自动检测与 `to(device)` 转移逻辑，同时在 `app.py` 中的 PCA 降维前向传播及单句推理前向传播中增加了 CPU 内存分离回收逻辑（使用 `.cpu().detach()`）。
- **阶段完成**：GPU 支撑适配完毕，运行日志显示已激活 CUDA 加速。

## 2026-02-25 (增量开发二)

- **目标**：实现模型权重的持久化存储（Save Model）。
- **当前进展**：在 `app.py` 中侧边栏添加了“保存模型到本地”的独立交互按钮。点击后会将当前的 `model_weights.pth` 和词表 `tokenizer.pkl` 落盘至新建的 `output/` 文件夹。

## 2026-02-25 (总结与规范制定)

- **目标**：为项目产出 `WORKSPACE_RULES.md` 规范说明书。
- **当前进展**：确立了 KISS 极简开发哲学与 Manus 风格规划机制，收束了文件职责并生成规范性文档。
- **阶段完成**：已产出该说明书，作为本项目后续维护的刚性约束。

## 2026-02-25 (增量开发三)

- **目标**：增加准确率监控指标与全局集相似度分布直方图，可视化模型分辨能力。
- **当前进展**：在 `train.py` 中增加了结合正负阈值的 Acc 计算；在 `app.py` 首页实现了 Loss/Acc 双轨曲线渲染，并在训练完成后使用 `matplotlib.pyplot` 直观地渲染绿色（相似）和红色（不相似）重叠直方分布。
- **阶段完成**：新指标与底层判定能力可视化全部搭建通过。

## 2026-02-26 (增量开发四)

- **目标**：新增 CNN 双塔模型，保留现有 MeanPooling 架构以便对比学习。
- **阶段完成**：CNN 双塔与 MeanPooling 极简双塔共存，前端可视化全部复用。

## 2026-02-26 (增量开发五)

- **目标**：模型架构适配 2w 大数据集；解决 Streamlit 界面卡顿与预测按钮失效问题。
- **当前进展**：
  1. **数据清洗与分词器升级**：在 `data.py` 中增加了滤除 HTML、标点和特殊符号的文本清洗。为 CNN 引入 `jieba` 构建了 `SimpleWordTokenizer` 词级分词器，词表规模扩大至 15773。
  2. **CNN 架构增强**：为了解决 10 轮后 Loss 反弹（过拟合）及余弦距离维度诅咒，为 CNN 增加了 `BatchNorm1d`、`Dropout(0.3)` 以及一个全连接 `Linear` 投影层将卷积拼接特征压缩回 128 维。增强后的 CNN 在 10 轮训练下 Loss 降至 0.3672，反超基线。
  3. **Streamlit 性能攻坚**：解决了 2w 数据集带来的高频 WebSocket 图表推送崩溃问题（增加了每 50 batch 节流刷新）；使用 `@st.cache_resource` 缓存模型加载；使用 `@st.cache_data` 配合 GPU `torch.no_grad()` 批量推理 (`batch_size=256`)，大幅加速全局相似度直方图的渲染，彻底解决了 Tab3 预测按钮遭重计算阻塞导致无法点击的 Bug。
- **阶段完成**：系统成功支撑 2w 规模数据集的训练与全功能高速交互体验。

## 2026-02-28 (增量开发六)

- **目标**：新增 LSTM 双塔模型，引入并解决深度序列模型在自然语言表示中的经典问题。
- **当前进展**：
  1. **架构设计**：在 `model.py` 中引入 `LSTMDualEncoder`。采用 `2层双向LSTM + 掩码平均池化` 的设计替代单层结构，增强深层语义捕获。LSTM 共享基于 `jieba` 的词级分词器。
  2. **缓解大词表梯度稀疏**：由于 15773 的大词表下大部分词频极低，导致 Embedding 梯度极其稀疏。引入了 `nn.LayerNorm` 紧接在 Embedding 之后，强行将所有词向量拉伸至统一的数据分布，确保了 LSTM 接收输入的稳定性。
  3. **解决高学习率梯度爆炸**：由于 BPTT (沿时间反向传播) 效应，LR=0.3 时 LSTM 出现了梯度指数爆炸导致 Loss 变为 NaN。在 `train.py` 中独立为 LSTM 加入了基于 `torch.nn.utils.clip_grad_norm_` 的梯度裁剪 (max_norm=1.0) 机制，安全阀完美解决了该问题。
  4. **解决表示坍塌 (Representation Collapse)**：在投影层使用 `tanh` 强行约束范围虽然能稳定初期的余弦距离，但在高 LR 下会导致激活值饱和至 ±1，使得所有句子的向量分布变得高度一致（相似度全为 ~1），正负样本直方图完全重叠。最终移除 `tanh` 恢复纯线性投影架构，重新打通了多方向特征表达，实现了远超 MeanPooling 和 CNN 的极致分离（10 epoch 测试 Loss 降至 0.1190，碾压级优势）。
- **阶段完成**：LSTM 与 CNN、MeanPooling 三足鼎立，全面展示了三种不同空间建模(词袋/局部卷积/全局时序)的核心差异与调优难点。

## 2026-03-01 (增量开发七)

- **目标**：引入全量 LCQMC 数据集 (`lcqmc_max`，约 24 万对)，并向下兼容 2w 和更小规模的语料库。
- **当前进展**：
  1. **数据源解耦**：在 `app.py` 中将静态的 `lcqmc_2w.csv` 路径彻底替换为基于 UI 选择的动态路径 `selected_dataset_path`，涵盖所有训练和可视化逻辑。
  2. **前端规模切换与自适应**：在侧边栏新增“训练数据规模”下拉选择框，支持在 `全量集 (lcqmc_max, 约24w条)`、`中型集 (lcqmc_2w, 约2w条)` 和 `迷你集 (lcqmc_mini, 60条)` 之间无缝热切换。
  3. **内置超参专家引导**：动态关联了各量级最佳推荐：大语料使用低 Epoch + 低 LR (0.0005) + 大 BatchSize 防过拟合与加速，小语料反之，大幅降低了用户的调参心智负担。
- **阶段完成**：Web UI 现已完整覆盖从小样本调试到工业级大规模预训练的任意平滑切换。

## 2026-03-01 (增量开发八)

- **目标**：模型架构终极演进（Optimize & Enhance），全面适配并强化对 24 万大数据的吸收能力。
- **当前进展**：
  1. **MeanPooling (极简双塔) 强化**：增加了 `nn.LayerNorm` 抑制大词表稀疏现象，并新增了一个 `nn.Linear` 投影层使极其朴素的词袋模型有了特征维度的变化与交互能力。
  2. **CNN 双塔强化**：同样补充了针对大词表的 `LayerNorm`，将其前置于多尺度卷积网络前，为高维词映射进行底层数值维稳。
  3. **LSTM 双塔终极强化 (Attention)**：彻底剥离了粗糙的“掩码取平均”池化策略，引入了经典的**注意力机制 (Self-Attention Pooling)** 对变长句子进行降维压缩。LSTM 提供时序隐状态，线性层+Tanh 提供重要性评估计算权重，最后进行Softmax加权求和。极大增强了文本中关键成分（特征词汇）的提取能力。
- **阶段完成**：三种经典模型已全部在工程侧完成了对抗大数据集的抗压强化。全量数据适配基本完成。

## 2026-03-01 (增量开发九: 向量库进阶与完结)

- **目标**：彻底解决 24 万大数据的每次热启动分词加载瓶颈，并实现毫秒级工业化检索能力。
- **当前进展**：
  1. **分词资产与张量序列化秒开**：通过在 `data.py` 添加 `data/cache/*.pkl` 离线缓存机制（基于 MD5 Hash 文件与超参标定），成功将预处理由几十秒至数分钟暴力压缩至 **1-3秒内**完成，真正做到工业级启动体验。
  2. **混合向量检索库 (Tab 4)**：在 `app.py` 中独立装载了向量集推演引擎。用户主动构建全量字典后提取近 40 万条 Unique Text 进入内存。
  3. **检索基准对照**：引入 GPU 驱动的 PyTorch 纯矩阵并行算法与 CPU `Faiss` 近似最邻近（ANN）引擎进行直接对比。在近 40 万库中搜索最相似 Top-10 语句，PyTorch 在百毫秒级完赛，而 FAISS 直接跑进 ~1ms 级。
  4. **全端能力补齐**：查询结果同步返回了原始文本环境中的 `Label` (正/负样本)，帮助学习者肉眼验证基于完全自监督对比与纯净语义计算后得出的结果正确性。
- **项目完结声明**：经过理论搭建、可视化重构、模型演进及工业落地四大核心脉络，第八阶段 (向量库进阶) 圆满完成。本极简 NLP Embedding 项目达到了最终全案定型！
