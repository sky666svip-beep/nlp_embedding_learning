# NLP Embedding 学习项目计划

## 目标
从0开始学习简单 NLP 语义 Embedding 模型，覆盖从训练（底层原理：手搭双塔结构等）到部署（交互式可视化 Web UI）的完整流程。
侧重于通过可视化和交互来直观学习模型训练过程和最终效果。
初始目标任务为计算句子两两相似度打分 (STS)，后续扩展简单检索/召回任务。

## 阶段规划

### 阶段一：理论与数据准备 (In Progress)
- [ ] 确定简单的模型架构（例如基础词典 Embedding + 平均池化层的极简双塔模型，或者极简的 Word2Vec 变体）。
- [x] 准备开源中文 STS 数据集的小部分子集（如 LCQMC Mini）。
- [ ] 构建 PyTorch/TensorFlow (这里倾向于用 PyTorch 因为更易于理解和手写) 的 `Dataset` 和 `DataLoader`。

### 阶段二：模型构建与训练逻辑实现 (Not Started)
- [ ] 手写简单 Embedding 层和网络结构代码（不使用预训练模型权重）。
- [ ] 实现损失函数（如 Cosine Embedding Loss 或 Contrastive Loss）。
- [ ] 编写训练循环（Training Loop）。
- [ ] 记录训练过程中的关键指标（Loss，并在几个预设样本上计算相似度），用于可视化。

### 阶段三：可视化与交互展示 (Complete)
- [x] 选择并设置可视化工具（Streamlit 或 Gradio，结合 matplotlib/plotly）。
- [x] 实现交互式训练可视化：展示 Loss 曲线及核心参数变化。
- [x] 实现训练后效果可视化：动态展示句子 Embedding 在二维空间（PCA/t-SNE 降维）的分布。
- [x] 实现交互推理：用户输入任意两个句子，模型实时打分预测它们的相似度。

### 阶段四：功能扩展与性能优化 (Ongoing)
- [x] 增加 GPU 加速功能增快训练速度。
- [x] 增加保存训练后模型的功能 (包含词表与权重)。
- [x] 增加训练准确率曲线监控。
- [x] 新增句子相似度分布直方图 (判定两极分化程度)。
- [x] 新增 CNN 双塔模型 (`CNNDualEncoder`) 并保留现有 MeanPooling 架构以便对比学习。
- [x] 新增 LSTM 双塔模型 (`LSTMDualEncoder`) 捕获全局序列依赖，作为深度对比基线。

### 阶段五：性能优化与除虫 (Phase 6)
- [x] 优化 `app.py` 性能，通过 `@st.cache_data` 与 `@st.cache_resource` 避免 2w 数据集带来的界面卡顿与按钮失效问题。
- [x] 完善文档，总结 CNN 增强及 2w 数据集优化过程。

### 阶段六：向量库进阶 (Pending)
- [ ] 建立一个微型向量库（暴力计算）。
- [ ] 实现 UI 界面：输入 Query，返回最相似 top-k 的句子并展示距离。

### 阶段五：开发增强文档 (Complete)
- [x] 在每次功能增强后，产出/更新 `docs/` 下的开发文档。

## 决策记录
| 决策 | 原因 | 状态 |
|------|------|------|
| 采用底层手搭模式 | 用户选择了选项 B，旨在“从0开始学习”底层原理。 | 确定 |
| 使用 PyTorch | 适合教学、手搭神经网络，计算图灵活直观。 | 待执行 |
| 初始任务为 STS | 用户明确要求先做 STS（计算两个句子语义相似度打分）。 | 确定 |
| 基于网页形式部署 | 用户要求“可视化见到训练过程和各种效果”，采用 Streamlit/Gradio 能最快打通训练图表与推理交互。 | 确定 |

## 错误记录
| 错误 | 发生阶段 | 解决方案 |
|------|----------|----------|
| (暂无) | - | - |
