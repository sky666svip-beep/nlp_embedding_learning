1. 训练参数该如何调整？
Epochs (训练轮数)
作用： 决定模型把完整的 60 条数据看多少遍。
调整建议： 如果你发现 Loss 曲线还在快速下降，说明模型还在学习阶段，可以增加 Epochs（比如调到 30-50）；
如果 Loss 曲线已经横向平缓甚至开始上下震荡，说明模型已经在这个小数据集上“见顶”甚至过拟合了，此时可以减小 Epochs。
Learning Rate (学习率)
作用： 决定模型每次察觉到错误时，纠正自身参数的幅度（步伐大小）。
调整建议： 如果你发现 Loss 曲线下降得极慢或者几乎不动，说明步子太小，可以调大（比如 0.01）；
如果你发现 Loss 折线图上蹿下跳，极度陡峭且不稳定，或者报出 NaN（无法计算的数字），说明步子太大“劈叉”了，必须调小（比如 0.001 或 0.0005）。
Batch Size (批次大小)
作用： 决定模型每次看多少句话后才进行一次反思和参数纠正。
调整建议： 我们总共只有 60 组句子。当你设置 Batch Size 为 60 时，模型就是看完全部数据再集体纠正 1 次（下降最平滑但较慢）；
如果你设为 4，它就是每看 4 组就纠正 1 次（Loss 曲线会存在较多毛刺，但更容易跳出局部误区）。建议在使用小数据集时保持在 8 或 16 即可。
词向量维度 (Embedding Dimension)
作用： 决定我们用多长的一串数字（维度空间）来代表一个“字”。比如维度 8，一个字就有 8 个特征数字；维度 128，就有 128 个特征数字。
调整建议： 理论上维度越高，模型能表达的语义越丰富。
但由于咱们的数据量只有微型的 60 条，给太高维度（如 256）不仅浪费算力，
还会导致模型“死记硬背”这 60 句话（严重过拟合），导致它对新句子的预测能力变差。
在这个教学微型模型上，保持在 16 到 64 是比较合理的。

---

## 一、什么是「拟合」？
**拟合 = 模型在“学习”数据里的规律**

你可以把：
- 训练数据 = 练习题
- 模型 = 学生
- 拟合 = 学生做题、学规律

### 拟合分三种情况：
1. **欠拟合（没学会）**
   - 连练习题都做不对
   - 训练 Loss 一直很高、降不下来
   → 模型太简单/学得太少，**啥都没学会**。

2. **正常拟合（学会了）**
   - 练习题做得好，新题（验证集）也做得好
   - 训练 Loss 下降，验证 Loss 跟着一起降，最后平稳
   → 学到了**真正的规律**。

3. **过拟合（死记硬背）**
   - 练习题背得滚瓜烂熟，但一考新题就崩
   - 训练 Loss 很低、还在震荡，验证 Loss 反而上升
   → 只记住了题目细节和噪音，**没学会通用规律**。


## 二、什么是「见顶」？
**见顶 = 模型已经学到极限，再也学不动了**

表现：
- 训练 Loss 不再下降，变成一条**横线**
- 验证 Loss 也平了，不再变好
- 再怎么继续训练，准确率/效果都**不再提升**

就像：
- 学生已经把能学的都学会了
- 再刷题、再熬夜，分数也不涨了
→ 这就是**见顶**。


## 一句话总结
- **拟合**：模型在学数据，学得好不好 = 拟合得好不好。
- **见顶**：模型学到头了，再训练也没用，性能到天花板了。

如果你用的是小数据集，**先见顶，再过拟合**是非常常见的现象。
## 解决办法
1）防 / 治过拟合
早停：验证 Loss 不再下降就立刻停止训练
数据增强：小样本最有效，扩充数据多样性
加简单正则：L2 正则、小概率 Dropout（别太大）
缩小模型：减少层数 / 神经元，别用大模型
2）让 “见顶” 的模型再涨点
调小学习率，精细再训
换优化器（Adam ↔ SGD）
清洗数据：删掉错标、噪声样本
补充少量高质量数据
## LOSS曲线
曲线快速往下掉
→ 模型在拼命学习，错误越来越少。
曲线变平，不再下降
→ 错误降不动了，就是你刚才问的 见顶。
训练曲线平了 / 震荡，验证曲线往上翘
→ 模型开始死记硬背，就是 过拟合。