import hashlib
import os
import pickle
import re
import jieba
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader

def clean_text(text):
    """è½»é‡çº§æ–‡æœ¬æ¸…æ´—ï¼šå»é™¤ HTML æ ‡ç­¾ã€æ ‡ç‚¹ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦ï¼Œåªä¿ç•™ä¸­æ–‡ã€å­—æ¯å’Œæ•°å­—"""
    text = str(text)
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'[^\u4e00-\u9fffA-Za-z0-9]', '', text)
    return text

class SimpleCharTokenizer:
    """å­—ç¬¦çº§åˆ†è¯å™¨ï¼šé€å­—æ‹†åˆ†ï¼Œé€‚åˆ MeanPooling æ¨¡å‹"""
    def __init__(self):
        self.vocab = {"[PAD]": 0, "[UNK]": 1}
        self.id2char = {0: "[PAD]", 1: "[UNK]"}
        
    def fit(self, texts):
        for text in texts:
            text = clean_text(text)
            for char in text:
                if char not in self.vocab:
                    idx = len(self.vocab)
                    self.vocab[char] = idx
                    self.id2char[idx] = char
                    
    def encode(self, text, max_len=32):
        text = clean_text(text)
        ids = [self.vocab.get(char, 1) for char in text]
        if len(ids) > max_len:
            ids = ids[:max_len]
        else:
            ids = ids + [0] * (max_len - len(ids))
        return ids

class SimpleWordTokenizer:
    """è¯çº§åˆ†è¯å™¨ï¼šä½¿ç”¨ jieba åˆ†è¯ï¼Œé€‚åˆ CNN æ¨¡å‹æ•è·è¯çº§ N-gram"""
    def __init__(self):
        self.vocab = {"[PAD]": 0, "[UNK]": 1}
        self.id2word = {0: "[PAD]", 1: "[UNK]"}
        
    def fit(self, texts):
        for text in texts:
            text = clean_text(text)
            for word in jieba.cut(text):
                if word not in self.vocab:
                    idx = len(self.vocab)
                    self.vocab[word] = idx
                    self.id2word[idx] = word
                    
    def encode(self, text, max_len=20):
        text = clean_text(text)
        words = list(jieba.cut(text))
        ids = [self.vocab.get(w, 1) for w in words]
        if len(ids) > max_len:
            ids = ids[:max_len]
        else:
            ids = ids + [0] * (max_len - len(ids))
        return ids

class STSDataset(Dataset):
    def __init__(self, df, tokenizer, max_len=32):
        self.tokenizer = tokenizer
        self.max_len = max_len
        self.data = [(clean_text(row['sentence1']), clean_text(row['sentence2']), int(row['label']))
                     for _, row in df.iterrows()]
        
    def __len__(self):
        return len(self.data)
        
    def __getitem__(self, idx):
        s1, s2, label = self.data[idx]
        id1 = torch.tensor(self.tokenizer.encode(s1, self.max_len), dtype=torch.long)
        id2 = torch.tensor(self.tokenizer.encode(s2, self.max_len), dtype=torch.long)
        label = torch.tensor(label, dtype=torch.float32)
        return id1, id2, label

def get_dataloader(csv_path, batch_size=16, tokenizer=None, max_len=32, tokenizer_type="char"):
    """tokenizer_type: 'char' (å­—ç¬¦çº§) æˆ– 'word' (è¯çº§ï¼Œjieba)
    åŠ å…¥æœ¬åœ°ç¦»çº¿ç¼“å­˜æœºåˆ¶ï¼Œå®ç°å¤§æ–‡ä»¶çš„ç§’çº§åºåˆ—åŒ–åŠ è½½ã€‚
    """
    # å»ºç«‹ç¼“å­˜æ ‡è¯†
    cache_dir = "data/cache"
    os.makedirs(cache_dir, exist_ok=True)
    
    file_stat = os.stat(csv_path)
    # åŸºäºè·¯å¾„ã€ä¿®æ”¹æ—¶é—´ã€åˆ†è¯ç±»å‹ç»„åˆå“ˆå¸Œ
    hash_str = f"{csv_path}_{file_stat.st_mtime}_{tokenizer_type}_{max_len}"
    cache_key = hashlib.md5(hash_str.encode()).hexdigest()
    
    dataset_cache_path = os.path.join(cache_dir, f"dataset_{cache_key}.pkl")
    tokenizer_cache_path = os.path.join(cache_dir, f"tokenizer_{cache_key}.pkl")
    
    if not tokenizer:
        if os.path.exists(dataset_cache_path) and os.path.exists(tokenizer_cache_path):
            print(f"ğŸ‘‰ å‘½ä¸­æœ¬åœ°ç¼“å­˜: {cache_key}ï¼Œæ­£åœ¨æé€ŸåŠ è½½åºåˆ—åŒ–æ•°æ®é›†...")
            with open(tokenizer_cache_path, "rb") as f:
                tokenizer = pickle.load(f)
            with open(dataset_cache_path, "rb") as f:
                dataset = pickle.load(f)
            return DataLoader(dataset, batch_size=batch_size, shuffle=True), tokenizer

        print(f"â³ æœªå‘½ä¸­æœ¬åœ°ç¼“å­˜: æ­£åœ¨å…¨é‡åˆ†è¯ä¸å¼ é‡åŒ– {csv_path} (è¯·è€å¿ƒç­‰å¾…)...")
        df = pd.read_csv(csv_path)
        
        if tokenizer_type == "word":
            tokenizer = SimpleWordTokenizer()
            max_len = 20  # è¯çº§åˆ†è¯ååºåˆ—æ›´çŸ­
        else:
            tokenizer = SimpleCharTokenizer()
        
        # æå–è¯­æ–™è®­ç»ƒåˆ†è¯å™¨
        texts = df['sentence1'].tolist() + df['sentence2'].tolist()
        tokenizer.fit(texts)
        
        # å®ä¾‹åŒ– Datasetï¼ˆä¼šåœ¨å†…éƒ¨è¿›è¡Œ encodeï¼‰
        dataset = STSDataset(df, tokenizer, max_len)
        
        # è½ç›˜ç¼“å­˜
        with open(tokenizer_cache_path, "wb") as f:
            pickle.dump(tokenizer, f)
        with open(dataset_cache_path, "wb") as f:
            pickle.dump(dataset, f)
            
        print(f"âœ… ç”Ÿæˆé™æ€ç¼“å­˜å®Œæˆ: {cache_key}")
    else:
        # å¦‚æœä»å¤–éƒ¨ä¼ å…¥äº†å·²è®­ç»ƒå¥½çš„ tokenizerï¼ˆä¾‹å¦‚é¢„æµ‹æ—¶ï¼‰ï¼Œåˆ™ç›´è¿ä¸ç¼“å­˜
        df = pd.read_csv(csv_path)
        dataset = STSDataset(df, tokenizer, max_len)

    return DataLoader(dataset, batch_size=batch_size, shuffle=True), tokenizer
