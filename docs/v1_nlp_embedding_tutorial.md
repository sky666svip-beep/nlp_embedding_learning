# NLP Embedding 极简双塔模型: 从 0 到 1 学习指南

## 🌟 背景与目标
为了帮助初学者直观理解自然语言处理 (NLP) 中的 Embedding（词嵌入）概念及其实现原理，我们剥离了繁杂的框架和海量的预训练数据，从零开始搭建了一个极简的“双塔模型”。
这个项目的重点在于**可视化与交互**：我们不追求最优的业务性能，而是通过 Streamlit 将模型训练的动态过程（Loss 下降）、高维特征映射过程（PCA 二维可视）和推理判断表现得淋漓尽致。

## 🧠 核心实现逻辑与设计思考

### 1. 数据集 (Dataset & Tokenization)
- **数据**：使用经典的开源中文相似度数据集（LCQMC）的一个微型子集 `lcqmc_mini.csv`（包含 60 条典型的相似/不相似问答样本）。
- **分词器（Tokenizer）**：手写了一个极为简单的 `SimpleCharTokenizer`。它通过一次遍历建立基于字 (Char-Level) 的词汇表，并在推断时处理填充 (Padding) 逻辑，展现“字到 ID 向量”的核心本质。
- **打包**：借助 PyTorch 标准的 `Dataset` 和 `DataLoader` 配置，体现经典深度学习工程规范。

### 2. 双塔模型 (Dual-Encoder Architecture)
- **极简结构**：模型 `SimpleDualEncoder` 仅包含一层基础的 `nn.Embedding`，它将离散文字转化为连续稠密向量。通过掩码（Mask）机制消除了填充 0 ID 的影响。
- **池化（Pooling）**：采用基础无参数的平均池化（Mean Pooling），把不定长的字符级序列特征浓缩成了单个定长的句子表示向量。
- **相似度判定**：两句经汇聚后的向量位于同一数值空间，直接使用 `F.cosine_similarity` （余弦相似度）计算它们的距离。

### 3. 训练与反馈
- 相似/不相似标签从 `[0, 1]` 被线性映射到 `[-1, 1]` 区间，与自然落在 `[-1, 1]` 范围的余弦相似度进行对齐，最后直接计算均方差损失（MSELoss）。
- 结合 Streamlit 独有的一点：通过 Callback 将每一个 Batch 的实时 Loss 上传至 `st.session_state`，能够**动态呈现于前端折线图上**，增强了学习与互动的乐趣。

### 4. 硬件加速 (GPU Support)
- 模型底层已在 `train.py` 和 `app.py` 中增加了对异构计算设备（如 NVIDIA CUDA 或 Apple Silicon MPS）的自动识别与接管。如果检测到可用硬件加速器，模型及运算张量会自动载入设备从而获得极大的训练速度增益。

### 5. 模型持久化 (Model Persistence)
- 增加了在内存中把玩后**落盘**的能力。训练出满意的曲线后，点击侧边栏的“💾 保存模型到本地”，它会将 PyTorch 模型权重张量 (`model_weights.pth`) 和自定义分词表字典 (`tokenizer.pkl`) 分离且安全地以二进制形式存入工作目录的 `output/` 文件夹下。

## 🪄 组件说明
本次核心构建引入了以下分离的模块结构：
- `data.py` - 轻量级文本读取与批处理预备队；
- `model.py` - 最裸露纯粹的数学计算映射模型；
- `train.py` - 模型训练工厂与优化循环封装；
- `app.py` - 全局调度指挥与 Streamlit 交互呈现门户。

## 🚀 如何使用
该项目提供了开箱即用的 Web App：
1. 确保已在环境中安装依赖：`pip install -r requirements.txt`。
2. 启动图形化服务：终端执行 `streamlit run app.py`。
3. 浏览器会自动跳至 `http://localhost:8501`：
   - **步骤 1**：在侧边栏按下【🚀 开始训练】，沉浸式观察 Loss 函数的下坡运动。
   - **步骤 2**：在主控 Tab 的【空间特征降维】页面观察具有相近语义的句子/词汇在经过 PCA 主成分分析后在平面上的集群落点。
   - **步骤 3**：切换至最终的【模型预测】页面，左右手互搏，随意输入句子检验微型双塔模型对自然语言的朴素感知力。
