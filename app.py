import streamlit as st
import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
from train import train_model
from model import get_model
from data import get_dataloader
import torch
import os
import pickle
import matplotlib.pyplot as plt
import time
import faiss

st.set_page_config(page_title="NLP Embedding å­¦ä¹ ", layout="wide")
st.title("NLP Embedding åŒå¡”æ¨¡å‹")

# Sidebar
st.sidebar.header("âš™ï¸ è®­ç»ƒå‚æ•°")
model_arch = st.sidebar.selectbox("æ¨¡å‹æ¶æ„", ["MeanPooling æç®€åŒå¡”", "CNN åŒå¡”", "LSTM åŒå¡”"], index=0)
model_type_map = {"MeanPooling æç®€åŒå¡”": "mean_pooling", "CNN åŒå¡”": "cnn", "LSTM åŒå¡”": "lstm"}
selected_model_type = model_type_map[model_arch]

dataset_scale = st.sidebar.selectbox("è®­ç»ƒæ•°æ®è§„æ¨¡", [
    "å…¨é‡é›† (lcqmc_max, çº¦24wæ¡)", 
    "ä¸­å‹é›† (lcqmc_2w, çº¦2wæ¡)", 
    "è¿·ä½ é›† (lcqmc_mini, 60æ¡)"
], index=0)

dataset_map = {
    "å…¨é‡é›† (lcqmc_max, çº¦24wæ¡)": "data/lcqmc_max.csv",
    "ä¸­å‹é›† (lcqmc_2w, çº¦2wæ¡)": "data/lcqmc_2w.csv",
    "è¿·ä½ é›† (lcqmc_mini, 60æ¡)": "data/lcqmc_mini.csv"
}
selected_dataset_path = dataset_map[dataset_scale]

st.sidebar.markdown("---")
st.sidebar.subheader("ğŸ’¡ æ¨èå‚æ•°è®¾ç½®")
if "max" in selected_dataset_path:
    st.sidebar.info("å¤§è¯­æ–™å»ºè®®ï¼šEpochs: 1-3 | LR: 0.0005 | Batch Size: 128æˆ–256")
    default_epochs, default_lr, default_batch = 2, 0.0005, 3
elif "2w" in selected_dataset_path:
    st.sidebar.info("ä¸­è¯­æ–™å»ºè®®ï¼šEpochs: 5-10 | LR: 0.001 | Batch Size: 64")
    default_epochs, default_lr, default_batch = 5, 0.001, 2
else:
    st.sidebar.info("å¾®è¯­æ–™å»ºè®®ï¼šEpochs: 15-30 | LR: 0.005 | Batch Size: 16")
    default_epochs, default_lr, default_batch = 15, 0.005, 0

epochs = st.sidebar.slider("Epochs", min_value=1, max_value=50, value=default_epochs)
lr = st.sidebar.number_input("Learning Rate", value=default_lr, format="%.4f")
batch_size = st.sidebar.selectbox("Batch Size", [16, 32, 64, 128, 256], index=default_batch)
embed_dim = st.sidebar.slider("è¯å‘é‡ç»´åº¦", min_value=8, max_value=256, value=128)

if "model_state" not in st.session_state:
    st.session_state.model_state = None
if "tokenizer" not in st.session_state:
    st.session_state.tokenizer = None
if "model_type" not in st.session_state:
    st.session_state.model_type = "mean_pooling"
if "loss_history" not in st.session_state:
    st.session_state.loss_history = []
if "acc_history" not in st.session_state:
    st.session_state.acc_history = []
# ç”¨ä¸€ä¸ªéšæœºæ•°æˆ–æ—¶é—´æˆ³ä½œä¸ºç¼“å­˜å¤±æ•ˆçš„ keyï¼ˆå½“æ–°è®­ç»ƒå®Œæˆæ—¶å¼ºåˆ¶åˆ·æ–°ç¼“å­˜ï¼‰
if "train_version" not in st.session_state:
    st.session_state.train_version = 0

tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“Š 1. è®­ç»ƒä¸è¯„ä¼°ç›‘æ§", "ğŸŒŒ 2. ç©ºé—´ç‰¹å¾é™ç»´", "ğŸ¤– 3. æ¨¡å‹é¢„æµ‹", "ğŸ” 4. å‘é‡æ£€ç´¢ä¸åº”ç”¨"])

with st.sidebar:
    start_train = st.button("ğŸš€ å¼€å§‹è®­ç»ƒ", use_container_width=True)
    
    st.divider()
    if st.session_state.model_state is not None:
        if st.button("ğŸ’¾ ä¿å­˜æ¨¡å‹åˆ°æœ¬åœ°", use_container_width=True):
            os.makedirs("output", exist_ok=True)
            # Save weights
            torch.save(st.session_state.model_state, "output/model_weights.pth")
            # Save tokenizer vocabulary simply using pickle
            with open("output/tokenizer.pkl", "wb") as f:
                pickle.dump(st.session_state.tokenizer, f)
            st.success("æ¨¡å‹æƒé‡ä¸è¯è¡¨å·²ä¿å­˜è‡³ `output/` ç›®å½•ï¼")

if start_train:
    st.session_state.loss_history = []
    st.session_state.acc_history = []
    st.session_state.train_version += 1  # è®­ç»ƒç‰ˆæœ¬åŠ ä¸€ï¼Œè®©æ—§å›¾è¡¨ç¼“å­˜å¤±æ•ˆ
    
    with tab1:
        st.subheader("æ¨¡å‹è®­ç»ƒä¸æ”¶æ•›çŠ¶æ€")
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # è®¡ç®—æ€» batch æ•°
        tok_type = "word" if selected_model_type in ("cnn", "lstm") else "char"
        mock_dl, mock_tk = get_dataloader(selected_dataset_path, batch_size=batch_size, tokenizer_type=tok_type)
        total_batches = len(mock_dl)
        total_steps = epochs * total_batches
        
        def train_callback(epoch, batch_idx, loss, batch_acc):
            st.session_state.loss_history.append(loss)
            st.session_state.acc_history.append(batch_acc)
            current_step = epoch * total_batches + batch_idx + 1
            progress_bar.progress(current_step / total_steps)
            # ä»…åœ¨æ¯ä¸ª epoch ç»“æŸæ—¶æ›´æ–°çŠ¶æ€æ–‡æœ¬
            if batch_idx == total_batches - 1:
                status_text.text(f"Epoch {epoch+1}/{epochs} å®Œæˆ | å¹³å‡Loss: {loss:.4f} | æœ«å°¾Acc: {batch_acc:.4f}")
            
        with st.spinner(f"æ¨¡å‹ ({model_arch}) æ­£åœ¨å­¦ä¹ è¯­ä¹‰åˆ†å¸ƒä¸­..."):
            model, tokenizer = train_model(selected_dataset_path, epochs, batch_size, lr, embed_dim, model_type=selected_model_type, callback=train_callback)
            st.session_state.model_state = model.state_dict()
            st.session_state.tokenizer = tokenizer
            st.session_state.model_type = selected_model_type
            
        # è®­ç»ƒç»“æŸåä¸€æ¬¡æ€§æ¸²æŸ“æœ€ç»ˆæ›²çº¿
        col_c1, col_c2 = st.columns(2)
        with col_c1:
            st.write("ğŸ“‰ **Loss æ›²çº¿**")
            st.line_chart(st.session_state.loss_history)
        with col_c2:
            st.write("ğŸ¯ **Accuracy æ›²çº¿**")
            st.line_chart(st.session_state.acc_history)
        st.success(f"ğŸ‰ è®­ç»ƒå®Œæˆï¼ˆ{model_arch}ï¼‰ï¼è¯·å‰å¾€å…¶ä»– Tab æŸ¥çœ‹æ•ˆæœã€‚")
else:
    with tab1:
        st.subheader("æ¨¡å‹è®­ç»ƒä¸æ”¶æ•›çŠ¶æ€")
        col_c1, col_c2 = st.columns(2)
        with col_c1:
            st.write("ğŸ“‰ **Loss (å‡æ–¹è¯¯å·®) æ›²çº¿**")
            if st.session_state.loss_history:
                st.line_chart(st.session_state.loss_history)
            else:
                st.info("ğŸ‘ˆ ç­‰å¾…æ•°æ®ã€‚")
        with col_c2:
            st.write("ğŸ¯ **Accuracy (æ‰¹æ¬¡å‡†ç¡®ç‡) æ›²çº¿**")
            if st.session_state.acc_history:
                st.line_chart(st.session_state.acc_history)
            else:
                st.info("ğŸ‘ˆ ç­‰å¾…æ•°æ®ã€‚")
        
        if st.session_state.loss_history:
            st.success("ğŸ‰ è¿™æ˜¯å½“å‰å­˜ç•™çš„æ¨¡å‹è®­ç»ƒèµ°åŠ¿ã€‚")
        else:
            st.info("ğŸ‘ˆ è¯·å…ˆåœ¨ä¾§è¾¹æ ç‚¹å‡»ã€å¼€å§‹è®­ç»ƒã€‘ã€‚")

@st.cache_resource
def get_cached_model(model_type, model_state_bytes, _tokenizer):
    """ç¼“å­˜åŠ è½½çš„æ¨¡å‹ï¼Œé¿å…æ¯æ¬¡é‡æ–°æ„å»ºå’ŒåŠ è½½æƒé‡ã€‚
    æ³¨æ„ï¼šmodel_state_bytes æ˜¯ä¸ºäº†è®© Streamlit è¯†åˆ«çŠ¶æ€å˜åŒ–ï¼Œ_tokenizer ç”¨ä¸‹åˆ’çº¿å¿½ç•¥ hash"""
    tk = _tokenizer
    mod = get_model(model_type, len(tk.vocab), embed_dim)
    # Streamlit ç¼“å­˜æœºåˆ¶è¦æ±‚å‚æ•°æ˜¯å¯å“ˆå¸Œçš„ï¼Œæ‰€ä»¥æˆ‘ä»¬å­˜çŠ¶æ€å­—å…¸æ—¶å¤–å±‚ç”¨ bytes æˆ–è€…åªè®©ä¸Šå±‚åˆ¤æ–­
    # è¿™é‡Œé€šè¿‡ cache_resource åªåœ¨é¦–æ¬¡æˆ–å†…å®¹å˜åŒ–æ—¶åˆå§‹åŒ–ä¸€æ¬¡
    import io
    buffer = io.BytesIO(model_state_bytes)
    state = torch.load(buffer, weights_only=True)
    mod.load_state_dict(state)
    mod.eval()
    return mod, tk

def get_loaded_model():
    if not st.session_state.model_state or not st.session_state.tokenizer:
        return None, None
    import io
    buffer = io.BytesIO()
    torch.save(st.session_state.model_state, buffer)
    mod, tk = get_cached_model(st.session_state.model_type, buffer.getvalue(), st.session_state.tokenizer)
    return mod, tk

device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')

@st.cache_data
def compute_similarity_distribution(version, _model, _tokenizer, dataset_path):
    """è®¡ç®—æ•°æ®é›†çš„å…¨å±€ç›¸ä¼¼åº¦åˆ†å¸ƒï¼Œé€šè¿‡ version å‚æ•°æ§åˆ¶åˆ·æ–°"""
    df = pd.read_csv(dataset_path)
    mod = _model.to(device)
    pos_sims, neg_sims = [], []
    
    # æ‰¹å¤„ç†æ‰¹é‡æ¨ç†ï¼Œå¤§å¹…æå‡é€Ÿåº¦ (å–ä»£åŸæ¥åŸºäº df.iterrows çš„å•æ¡æ¨ç†)
    batch_size = 256
    labels = torch.tensor(df['label'].values)
    
    for i in range(0, len(df), batch_size):
        batch_df = df.iloc[i:i+batch_size]
        s1_batch = [torch.tensor(_tokenizer.encode(s, max_len=32)) for s in batch_df['sentence1']]
        s2_batch = [torch.tensor(_tokenizer.encode(s, max_len=32)) for s in batch_df['sentence2']]
        
        id1 = torch.stack(s1_batch).to(device)
        id2 = torch.stack(s2_batch).to(device)
        
        with torch.no_grad():
            sim, _, _ = mod(id1, id2)
        sim_val = sim.cpu().numpy()
        
        batch_labels = labels[i:i+batch_size].numpy()
        pos_sims.extend(sim_val[batch_labels == 1])
        neg_sims.extend(sim_val[batch_labels == 0])
        
    return pos_sims, neg_sims

with tab1:
    st.divider()
    st.subheader("ğŸ”¬ å…¨å±€é¢„æµ‹åˆ†å¸ƒé€è§†å›¾ (ç›¸ä¼¼åº¦åˆ†å¸ƒç›´æ–¹å›¾)")
    if st.session_state.model_state:
        mod, tk = get_loaded_model()
        
        with st.spinner("è®¡ç®—å…¨å±€ç›¸ä¼¼åº¦åˆ†å¸ƒ (å·²åˆ©ç”¨ GPU æ‰¹å¤„ç†æé€Ÿ & æ•°æ®ç¼“å­˜)..."):
            pos_sims, neg_sims = compute_similarity_distribution(st.session_state.train_version, mod, tk, selected_dataset_path)
            
            fig, ax = plt.subplots(figsize=(8, 3))
            ax.hist(pos_sims, bins=np.linspace(-1, 1, 21), alpha=0.6, label='Similar (Label=1)', color='green')
            ax.hist(neg_sims, bins=np.linspace(-1, 1, 21), alpha=0.6, label='Dissimilar (Label=0)', color='red')
            ax.set_title('Cosine Similarity Distribution Analysis')
            ax.set_xlabel('Cosine Similarity Score')
            ax.set_ylabel('Frequency')
            ax.legend(loc='upper right')
            st.pyplot(fig)
            st.markdown("ğŸ’¡ **æ€ä¹ˆçœ‹è¿™å¼ å›¾ï¼Ÿ** ç»¿è‰²ç›´æ–¹å›¾è¶Šé å³ (è¶‹è¿‘ 1)ï¼Œçº¢è‰²ç›´æ–¹å›¾è¶Šé å·¦ (è¶‹è¿‘ -1)ï¼Œé‡å éƒ¨åˆ†è¶Šå°‘ï¼Œè¯´æ˜æ¨¡å‹â€œåŒºåˆ†å¥å­ç›¸ä¼¼ä¸å¦çš„èƒ½åŠ›â€è¶Šå¼ºï¼ˆä¸¤æåˆ†åŒ–è¶Šå¥½ï¼‰ï¼")
    else:
        st.info("å°šæœªå®Œæˆè®­ç»ƒï¼Œæ— æ³•æŸ¥çœ‹åˆ†å¸ƒç›´æ–¹å›¾ã€‚")

@st.cache_data
def get_pca_data(version, _model, _tokenizer, sentences):
    """ç¼“å­˜ PCA é™ç»´åæ ‡"""
    mod = _model.to(device)
    embs = []
    with torch.no_grad():
        for s in sentences:
            ids = torch.tensor([_tokenizer.encode(s, max_len=32)], dtype=torch.long).to(device)
            vec = mod.encode_single(ids)
            embs.append(vec.squeeze(0).cpu().numpy())
    coords = PCA(n_components=2).fit_transform(np.array(embs))
    return coords

with tab2:
    st.subheader("äºŒç»´å¥å­ç©ºé—´åˆ†å¸ƒ (PCAé™ç»´)")
    st.markdown("å°†é«˜ç»´çš„å¥å­å‘é‡å‹ç¼©è‡³2Då¹³é¢ï¼Œè·ç¦»ç›¸è¿‘çš„ç‚¹ä»£è¡¨æ¨¡å‹è®¤ä¸ºå®ƒä»¬è¯­ä¹‰ç›¸ä¼¼ã€‚")
    if st.session_state.model_state:
        mod, tk = get_loaded_model()
        df = pd.read_csv(selected_dataset_path)
        # æå–å‰ 30 å¯¹å¥å­ç”¨äºå±•ç¤º
        sentences = list(set(df['sentence1'].tolist()[:30] + df['sentence2'].tolist()[:30]))
        
        with st.spinner("æŠ½å–é«˜ç»´ç‰¹å¾å¹¶è®¡ç®— PCA..."):
            coords = get_pca_data(st.session_state.train_version, mod, tk, sentences)
            chart_df = pd.DataFrame({"X": coords[:, 0], "Y": coords[:, 1], "Text": sentences})
            
            # Use columns to lay out side-by-side
            c1, c2 = st.columns([2, 1])
            with c1:
                st.scatter_chart(chart_df, x="X", y="Y")
            with c2:
                st.dataframe(chart_df[["X", "Y", "Text"]], use_container_width=True)
                st.caption("ğŸ‘ˆ å·¦ä¾§å›¾è¡¨ä¸ºç‚¹ä½åˆ†å¸ƒï¼Œå¯¹ç…§æ­¤è¡¨å¯æŸ¥æ‰¾å…·ä½“å¥å­æ‰€åœ¨åæ ‡ã€‚")
    else:
        st.info("ğŸ‘ˆ è¯·å…ˆåœ¨ä¾§è¾¹æ ç‚¹å‡»ã€å¼€å§‹è®­ç»ƒã€‘ã€‚")

with tab3:
    st.subheader("è¾“å…¥ä¸¤å¥è¯ï¼Œé¢„æµ‹ç›¸ä¼¼åº¦æŒ‡æ•°")
    mod, tk = get_loaded_model()
    
    col1, col2 = st.columns(2)
    s1 = col1.text_input("ç¬¬ä¸€å¥è¯", "è‹¹æœæ‰‹æœºæ€ä¹ˆæˆªå›¾")
    s2 = col2.text_input("ç¬¬äºŒå¥è¯", "iPhoneå±æ˜¾æ€ä¹ˆæˆª")
        
    if st.button("âš¡ è®¡ç®—ç›¸ä¼¼åº¦", type="primary"):
        if mod:
            mod = mod.to(device)
            id1 = torch.tensor([tk.encode(s1, max_len=32)], dtype=torch.long).to(device)
            id2 = torch.tensor([tk.encode(s2, max_len=32)], dtype=torch.long).to(device)
            sim, _, _ = mod(id1, id2)
            similarity_score = sim.cpu().item()
            
            st.metric(label="Cosine Similarity (ä½™å¼¦ç›¸ä¼¼åº¦)", value=f"{similarity_score:.4f}")
            if similarity_score > 0.5:
                st.success("ğŸ’¡ åˆ¤æ–­ï¼šå®ƒä»¬å¤§æ¦‚ç‡æ˜¯ **ç›¸ä¼¼** çš„ï¼")
            else:
                st.warning("â³ åˆ¤æ–­ï¼šå®ƒä»¬å¯èƒ½ **ä¸ç›¸ä¼¼**ã€‚")
        else:
            st.error("âš ï¸ æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè®­ç»ƒï¼")

with tab4:
    st.subheader("æ„å»ºæœ¬åœ°å¾®å‹å‘é‡åº“ä¸åŠ é€Ÿæ£€ç´¢ä½“éªŒ")
    st.markdown("å°†å…¨é‡æ•°æ®é›†çš„å¥å­æ˜ å°„æˆç‰¹å¾å‘é‡å­˜å…¥â€œå‘é‡åº“â€ï¼Œä½“éªŒé€šè¿‡ä¸€å¥è¯ç¬é—´æ‰¾å›æœ€ç›¸ä¼¼çš„ Top-K æ¡è¯­æ–™ã€‚é€šè¿‡å¯¹æ¯”æ™®é€šã€ŒPyTorch æš´åŠ›çŸ©é˜µç®—åˆ†ã€ä¸å¤§å‚ä½¿ç”¨çš„ã€ŒFAISS è¿‘ä¼¼æ£€ç´¢å¼•æ“ã€ï¼Œç›´è§‚æ„Ÿå—æ€§èƒ½å·®å¼‚ã€‚")
    
    col_v1, col_v2 = st.columns([1, 2])
    with col_v1:
        st.info("ğŸ’¡ å¿…é¡»å…ˆè®­ç»ƒæˆ–åŠ è½½æ¨¡å‹ï¼Œæ‰èƒ½å°†å…¶èƒ½åŠ›ç”¨äºæ„å»ºå‘é‡åº“ã€‚")
        build_db_btn = st.button("ğŸš€ æ ¹æ®å½“å‰è¯­æ–™åº“æ„å»ºå…¨é‡å‘é‡ç´¢å¼•", type="primary", use_container_width=True)
    
    if "vector_db" not in st.session_state:
        st.session_state.vector_db = None
    
    if build_db_btn:
        if not st.session_state.model_state:
            st.error("âš ï¸ å½“å‰æ— å¯ç”¨æ¨¡å‹ï¼è¯·å…ˆåœ¨ä¾§æ‹‰æ ç‚¹å‡»ã€å¼€å§‹è®­ç»ƒã€‘ã€‚")
        else:
            mod, tk = get_loaded_model()
            mod = mod.to(device)
            df = pd.read_csv(selected_dataset_path)
            
            with st.spinner("1/3 æ­£åœ¨æå–å”¯ä¸€å¥å­å­—å…¸ (è¿‡æ»¤é‡å¤è¯­å¥)..."):
                # ä¸ºäº†é˜²æ­¢ä¸¤ä¸¤é…å¯¹æ•°æ®é‡Œå¤§é‡é‡å¤å¥å­å ç”¨æ˜¾å­˜ï¼Œé¦–å…ˆå»é‡
                s1_unique = df[['sentence1', 'label']].rename(columns={'sentence1': 'text'})
                s2_unique = df[['sentence2', 'label']].rename(columns={'sentence2': 'text'})
                # ç”±äºç›¸åŒçš„å¥å­å¦‚æœåœ¨åŸè¡¨ä¸­é‡åˆ°è¿‡æ­£æ ·æœ¬åˆé‡åˆ°è´Ÿæ ·æœ¬ï¼Œä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°çš„ï¼Œä»…ä½œä¸ºæ£€ç´¢ç»“æœå‚è€ƒ
                unique_df = pd.concat([s1_unique, s2_unique]).drop_duplicates(subset=['text'], keep='first').reset_index(drop=True)
                sentences = unique_df['text'].tolist()
                labels = unique_df['label'].tolist()
            
            with st.spinner(f"2/3 æ­£åœ¨ä½¿ç”¨ GPU å°† {len(sentences)} æ¡è¯­å¥ç¼–ç ä¸ºé«˜ç»´ç‰¹å¾ (å¯èƒ½è€—æ—¶æ•°åç§’)..."):
                all_embeddings = []
                batch_size = 256
                # æ­¤å¤„ä¹Ÿåº”ç”¨æ‰¹é‡å¤„ç†åŠ é€Ÿ
                for i in range(0, len(sentences), batch_size):
                    batch_texts = sentences[i:i+batch_size]
                    # Tokenize
                    ids = [torch.tensor(tk.encode(s, max_len=32)) for s in batch_texts]
                    input_tensors = torch.stack(ids).to(device)
                    with torch.no_grad():
                        embs = mod.encode_single(input_tensors)
                        embs = torch.nn.functional.normalize(embs, p=2, dim=1) # å¿…é¡» L2 æ ‡å‡†åŒ–ä»¥è®¡ç®—ä½™å¼¦/å†…ç§¯
                    all_embeddings.append(embs.cpu())
                    
                full_tensor = torch.cat(all_embeddings, dim=0) # [N, embed_dim]
                
            with st.spinner("3/3 æ­£åœ¨æ„å»º FAISS å€’æ’ç´¢å¼•..."):
                np_embeddings = full_tensor.numpy().astype('float32') # FAISS å¼ºä¾èµ– float32 numpy æ ¼å¼
                # ä½¿ç”¨å†…ç§¯ (Inner Product) ç´¢å¼•è¯„ä¼°ï¼Œå› ä¸ºæ ‡å‡†åŒ–åå†…ç§¯ == ä½™å¼¦ç›¸ä¼¼åº¦
                faiss_index = faiss.IndexFlatIP(embed_dim) 
                faiss_index.add(np_embeddings)
                
            # ä¿å­˜åˆ° session
            st.session_state.vector_db = {
                "tensor": full_tensor.to(device), # æš´åŠ›çŸ©é˜µæœç´¢ç”¨
                "faiss": faiss_index,             # FAISS æŸ¥è¡¨æœç´¢ç”¨
                "texts": sentences,
                "labels": labels
            }
            st.success(f"ğŸ‰ æˆåŠŸæ„å»ºå‘é‡åº“ï¼ä¸€å…±ç¼–å…¥ {len(sentences)} æ ¹ä¸é‡å¤å‘é‡ã€‚")

    if st.session_state.vector_db:
        st.divider()
        st.markdown("### ğŸ” å¼€å§‹è¯­ä¹‰æ£€ç´¢å¯»æ‰¾ç›¸ä¼¼è¯­å¥")
        
        c_q1, c_q2 = st.columns([3, 1])
        with c_q1:
            query = st.text_input("è¯·è¾“å…¥æƒ³è¦å¯»æ‰¾ç›¸ä¼¼å¥å­çš„æ£€ç´¢è¯­å¥ (Query):", "åŒ—äº¬å¤©æ°”å’‹æ ·")
        with c_q2:
            top_k = st.slider("æƒ³è¦å¬å›çš„æ•°é‡ (Top-K):", min_value=1, max_value=50, value=10)
            
        if st.button("å¼€å§‹åŒè½¨å¹³è¡Œæ£€ç´¢", use_container_width=True):
            mod, tk = get_loaded_model()
            mod = mod.to(device)
            # 1. ç¼–ç ç”¨æˆ·çš„æŸ¥è¯¢è¯­å¥
            id_query = torch.tensor([tk.encode(query, max_len=32)], dtype=torch.long).to(device)
            with torch.no_grad():
                q_emb = mod.encode_single(id_query)
                q_emb = torch.nn.functional.normalize(q_emb, p=2, dim=1)
                
            db = st.session_state.vector_db
            
            # è½¨ 1: PyTorch çº¯çŸ©é˜µæš´åŠ›ç‚¹ä¹˜ç®—åˆ†
            pt_start = time.time()
            # q_emb [1, D], db['tensor'] [N, D] -> åˆ†æ•° [1, N]
            all_scores = torch.matmul(q_emb, db['tensor'].t()) 
            top_scores, top_indices = torch.topk(all_scores, k=top_k, dim=1)
            pt_end = time.time()
            pt_time_ms = (pt_end - pt_start) * 1000
            
            # è½¨ 2: FAISS é«˜é€Ÿç›¸ä¼¼åº¦æŸ¥è¡¨ç®—åˆ†
            np_q = q_emb.cpu().numpy().astype('float32')
            faiss_start = time.time()
            f_scores, f_indices = db['faiss'].search(np_q, top_k)
            faiss_end = time.time()
            faiss_time_ms = (faiss_end - faiss_start) * 1000
            
            st.markdown(f"**âš¡ æ£€ç´¢æ€§èƒ½å¯¹æ¯”:** `PyTorch:` **{pt_time_ms:.2f} æ¯«ç§’** vs  `FAISS:` **{faiss_time_ms:.2f} æ¯«ç§’**")
            # é€šè¿‡å¤§è¯­æ–™ (lcqmc_max 24w å»é‡åå¤§æ¦‚ 38 ä¸‡ unique å¥å­)ï¼Œå¯è§‚å¯Ÿåˆ°ä¸¤è€…å·¨å¤§å·®å¼‚
            
            # ç»„è£…å¬å›ç»“æœä¸ºç›´è§‚è¡¨æ ¼
            results = []
            f_idx_list = f_indices[0]
            f_score_list = f_scores[0]
            for r_i, (idx, score) in enumerate(zip(f_idx_list, f_score_list)):
                results.append({
                    "æ’å (Rank)": r_i + 1,
                    "å¬å›çš„å¥å­ (Sentence)": db['texts'][idx],
                    "ä½™å¼¦ç›¸ä¼¼åº¦": f"{score:.4f}",
                    "åŸå§‹è¯­å¢ƒåˆ¤å®š(Label)": "ç”±äºç¯å¢ƒå…³è” (1)" if db['labels'][idx] == 1 else "ç¯å¢ƒæ— å…³ (0)"
                })
                
            st.dataframe(pd.DataFrame(results), use_container_width=True)

