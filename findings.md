# 研究与发现记录

## 当前探索方向
- 极简 STS 语义相似度计算的底层模型架构。
- 适合教学与可视化的训练监控方法。

## 发现与结论
*(在此记录关于架构、数据集和 API 设计的启发点)*
- **语料库选择**：决定选用开源中文相似度数据集（如 LCQMC 的一个小部分子集）作为训练数据，而非手工构建极小语料。这样可以基于更真实的语言分布验证学习效果。
- **CNN 与 MeanPooling 对比实验 (2026-02-26)**：在同一 2w 数据集、同参数下，MeanPooling 极简双塔的 Accuracy 和两极分化效果**优于** CNN 双塔。
  - **根因分析**：当前使用字符级分词，CNN 的卷积核捕获的是单字组合 ("打","篮")，而非词级搭配 ("打篮球","运动")。字符级 N-gram 语义信号太弱，CNN 的局部特征提取能力被浪费。
  - **额外因素**：CNN 参数量更大容易过拟合；输出维度 192 > 128 加剧余弦相似度的维度诅咒。
  - **结论**：模型架构与分词粒度必须匹配。CNN 需要词级分词才能发挥优势。
  - **解决方案**：引入 jieba 词级分词器，CNN 选用词级分词，MeanPooling 保留字符级作为对比基线。

## 待探索的问题
1. 词汇表（Vocabulary）怎么处理？因为是手搭模型，我们可能需要自己实现 Tokenizer 并建立一套简单的字/词级到 ID 的映射。
2. 词级分词后 max_len 应设为多少？（中文句子平均 8-15 个词，max_len=20 应该足够）

